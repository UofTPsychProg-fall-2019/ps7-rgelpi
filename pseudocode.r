# Data analysis pipeline for the output of a Sequential Monte Carlo (aka particle filter) model which develops a posterior probability over rules explaining the data observed.

# First, the model is read in from WebPPL (a probabilistic programming language in which the model is written)
chain = {...} # this code is already mostly written and is included at the end of this pseudocode document

# Collects the Monte Carlo chains for a given point in time after collecting a certain number of observations. The posterior probability of rules generated by the chain will be returned from the function so that it can be visualized later. webppl is a function used by rwebppl to import an item of WebPPL code into R.
run = function(parameters, condition, n_observations){ 
	chains = webppl(chain, parameters, condition, n_observations) 
	for (rule : chains){
		# Rules will either be preselected or the most frequent will be saved
		return sum(rule)
		}
	}
		
# Set up the parameters and condition used for this run.
parameters = c(...) # A list of four numbers used to represent input parameters for the model.
condition = "near" or "distant"

# Run the model from 0 to 10 observations, to observe its change over time after being exposed to new data.
for (int i = 0; i <= 10; i++){
	add_to_df(run(parameters, condition, i))
	}

#  Plot the rules as a line graph over time. Rules that better fit the data will become more probable over time and those that are a poor fit for the data will ideally become less probable, unless the model is unable to discover a better rule with the limitations placed on it by the particle filter.
plot_line_graph(x = n_observations, y = probability, line = rule)

# Next, compare the output of this rule distribution, as well as alternative explanations such as deterministic rule application, to humans' judgments, using log-likelihood calculation, to see if humans' judgments are better captured by a model with a marginal distribution of rules, or by a single rule. To start, we read in a dataframe containing all 32 blocks a participant could encounter...
all_blocks = read_csv(blocks.csv)

# ...as well as the judgments expected by some rules.
for (rule : rules){
	return isBlicket(block) ? 1-noise : 0+noise # noise parameter to prevent log-likelihood from being negative infinity
	}
	
# Next, tweak the model to output the probability that any given block is likely to activate a machine according to the posterior rule distribution...
chain = {...}

# ...and the helper function that adds this to a data frame so it can be manipulated in r
collect_blocks = function(parameters, condition){
	chains = webppl(chain, parameters, condition, 10) # n_observations = 10
	for (block : chains){
		return(block, probability(block))
		}
	}
	
# Apply the rules to get the log probability of human judgments under a given rule framework.	
human_judgments = read_csv(human.csv)
apply_rule = function(rule){
	probability = rule.probability # Pick the right rule to apply
	for (block : human_judgments){
		# Get the log probability of a correct judgment of blicket or nonblicket
		log_prob = isBlicket(block) ? log(probability(block)) : log(1-probability(block))
		}
	return sum(log_prob)
	}

# Apply rule for deterministic rules as well as the model
apply_rule(rule)
apply_rule(collect_blocks)

# Finally, generate an imaginary "participant pool" from the model's output, based on humans' judgments.
for (block : human_judgments){
	model_preds = isBlicket(block) ? probability(block) : 1-probability(block)
	}
# This can then be plotted as a bar graph alongside adults' results

plot_bar_graph(x = blicketConsistency, y = score, fill = nonBlicketConsistency) +
	facet_grid(~.condition)