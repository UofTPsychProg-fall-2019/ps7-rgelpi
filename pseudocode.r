# Data analysis pipeline for the output of a Sequential Monte Carlo (aka particle filter) model which develops a posterior probability over rules explaining the data observed.

# First, the model is read in from WebPPL (a probabilistic programming language in which the model is written)
chain = {...} # this code is already mostly written and is included at the end of this pseudocode document

# Collects the Monte Carlo chains for a given point in time after collecting a certain number of observations. The posterior probability of rules generated by the chain will be returned from the function so that it can be visualized later. webppl is a function used by rwebppl to import an item of WebPPL code into R.
run = function(parameters, condition, n_observations){ 
	chains = webppl(chain, parameters, condition, n_observations) 
	for (rule : chains){
		# Rules will either be preselected or the most frequent will be saved
		return sum(rule)
		}
	}
		
# Set up the parameters and condition used for this run.
parameters = c(...) # A list of four numbers used to represent input parameters for the model.
condition = "near" or "distant"

# Run the model from 0 to 10 observations, to observe its change over time after being exposed to new data.
for (int i = 0; i <= 10; i++){
	add_to_df(run(parameters, condition, i))
	}

#  Plot the rules as a line graph over time. Rules that better fit the data will become more probable over time and those that are a poor fit for the data will ideally become less probable, unless the model is unable to discover a better rule with the limitations placed on it by the particle filter.
plot_line_graph(x = n_observations, y = probability, line = rule)

# Next, compare the output of this rule distribution, as well as alternative explanations such as deterministic rule application, to humans' judgments, using log-likelihood calculation, to see if humans' judgments are better captured by a model with a marginal distribution of rules, or by a single rule. To start, we read in a dataframe containing all 32 blocks a participant could encounter...
all_blocks = read_csv(blocks.csv)

# ...as well as the judgments expected by some rules.
for (rule : rules){
	return isBlicket(block) ? 1-noise : 0+noise # noise parameter to prevent log-likelihood from being negative infinity
	}
	
# Next, tweak the model to output the probability that any given block is likely to activate a machine according to the posterior rule distribution...
chain = {...}

# ...and the helper function that adds this to a data frame so it can be manipulated in r
collect_blocks = function(parameters, condition){
	chains = webppl(chain, parameters, condition, 10) # n_observations = 10
	for (block : chains){
		return(block, probability(block))
		}
	}
	
# Apply the rules to get the log probability of human judgments under a given rule framework.	
human_judgments = read_csv(human.csv)
apply_rule = function(rule){
	probability = rule.probability # Pick the right rule to apply
	for (block : human_judgments){
		# Get the log probability of a correct judgment of blicket or nonblicket
		log_prob = isBlicket(block) ? log(probability(block)) : log(1-probability(block))
		}
	return sum(log_prob)
	}

# Apply rule for deterministic rules as well as the model
apply_rule(rule)
apply_rule(collect_blocks)

# Finally, generate an imaginary "participant pool" from the model's output, based on humans' judgments.
for (block : human_judgments){
	model_preds = isBlicket(block) ? probability(block) : 1-probability(block)
	}
# This can then be plotted as a bar graph alongside adults' results

plot_bar_graph(x = blicketConsistency, y = score, fill = nonBlicketConsistency) +
	facet_grid(~.condition)

# Below is an example of the webppl function (this uses the Markov chain Monte Carlo sampling method, but the output will be of the same form for SMC.) The parameters here are fixed, but can be changed through the use of the rwebppl tools.

chains = "
var rule_dist = Infer({method: 'MCMC', samples:25000, lag:2, burn:5000},  function(){
  var tau = 0.4
  var e_ne_prob = 0.2
  var noiseparam = 0.001
   //Dirichlet prior on feature distribution, with A being more likely
   //phi is the distribution over features (the entries in phi sum to 1) 
  var phi = dirichlet(Vector([4, 1, 1, 1, 1]))
  //uncomment for uniform prior on feature distribution
  //var phi = dirichlet(Vector([1, 1, 1, 1, 1]))
  var featureDist = Categorical({ps: phi, vs: ['A','B', 'C', 'D', 'E']})
  var samplePred = function() {
    if (flip(e_ne_prob)) {
      var trait1 = sample(featureDist)
      var trait2 = sample(featureDist)
      var op = flip() ? '==' : '!='
      return [op, trait1, trait2]
    } 
    else {
      var trait = sample(featureDist)
      var value = flip() ? 'true' : 'false'
      return ['==', trait, value]
    }
  }
  // generates either a predicate or an and between two predicates or conjunctions
  var sampleConj = function() {
    if(flip(tau)) {
      var c = sampleConj()
      var p = samplePred()
      return ['&&', c, p]
    } else {
      return samplePred()
    }
  }
  // the base function for generating rules, this generates either a conjugate expression
  // or disjunction between conjugate expressions
  var getFormula = function() {
    if(flip(tau)) {
      var c = sampleConj()
      var f = getFormula()
      return ['||', c, f]
    } else {
      return sampleConj()
    }
  }
  // sample a rule from the prior
  var e = getFormula();
  // make the rule look nice (for the table)
  var s = prettify(e);
  // turn the rule into a function that can be run on blocks
  var f = runify(e);
  //var f_fuzzy = function(x) {return f(x) ? flip(noiseparam) : flip(1-noiseparam)}
  // condition on all the training blocks having the same label (up to noise)
  var rule_outputs = map(f, training_blocks);
  var condition_on_equality = function(rule_output, label) {
    observe(Bernoulli({p: rule_output ? (1-noiseparam) : noiseparam}), label);
  };
  var rule_outputs = map(f, training_blocks);
  var n_obs = 10;
  map2(condition_on_equality, rule_outputs.slice(0,n_obs), training_labels.slice(0,n_obs));
//This simply returns the rule strings, but it can be edited to return a marginal rule distribution, the block probabilities, etc.
  return s
})"